{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies/libraries\n",
    "* nltk\n",
    "* numpy\n",
    "* bs4 (Beautifulsoup4)\n",
    "* sklearn\n",
    "\n",
    "`pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4 as bs\n",
    "\n",
    "html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Eiffel_Tower')\n",
    "html = html.read()\n",
    "\n",
    "article = bs.BeautifulSoup(html, 'lxml')\n",
    "paragraphs = article.find_all('p')\n",
    "\n",
    "text = ''\n",
    "for paragraph in paragraphs:\n",
    "    text += paragraph.text\n",
    "    \n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "text = re.sub(r'\\[[0-9]*\\]', ' ', text) # remove special characters\n",
    "text = re.sub(r'\\s+', ' ', text) # replace multiple spaces with single space\n",
    "\n",
    "sentences = nltk.sent_tokenize(text) # split the text to sentences\n",
    "words = nltk.word_tokenize(text) # split the text to words\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "punc_remover = dict((ord(punc), None) for punc in string.punctuation)\n",
    "\n",
    "def preprocess(document):\n",
    "    return lemmatize(nltk.word_tokenize(document.lower().translate(punc_remover)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules: Greetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "greeting_inputs = (\"hey\", \"good morning\", \"good evening\", \"morning\", \"evening\", \"hi\", \"whatsup\")\n",
    "greeting_responses = [\"hey\", \"hello\", \"hello, there!\", \"how can I help you?\"]\n",
    "\n",
    "def greeting_response(greeting):\n",
    "    for token in greeting.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules: User Queries\n",
    "\n",
    "Response is generated based upon the **cosine similarity** of the **tfidf vectorized** form of the input sentence and the sentences in the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def query_response(query):\n",
    "    \n",
    "    bot_response = ''\n",
    "    \n",
    "    sentences.append(query) # append user query to already existing sentences\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(tokenizer=preprocess, stop_words='english')\n",
    "    vectors = vectorizer.fit_transform(sentences) # vectorize all sentences\n",
    "    \n",
    "    # find the similarity between the user input (last item) and all the sentences\n",
    "    similar_vector_values = cosine_similarity(vectors[-1], vectors)\n",
    "    \n",
    "    # find the most similar sentence to the user input (second-last of the sorted list)\n",
    "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
    "    \n",
    "    # flatten and extract the matched vector\n",
    "    matched_vectors = similar_vector_values.flatten()\n",
    "    matched_vectors.sort()\n",
    "    vector_matched = matched_vectors[-2]\n",
    "    \n",
    "    if vector_matched == 0: # answer to query not found\n",
    "        bot_response += \"I am sorry, I could not understand that\"\n",
    "        return bot_response\n",
    "    else: \n",
    "        bot_response += sentences[similar_sentence_number]\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatting with the Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: Hello! You can ask me anything about the Eiffel Tower.\n",
      "Hi\n",
      "BOT:  how can I help you?\n",
      "constructed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT:  prior to the eiffel tower's construction, no structure had ever been constructed to a height of at least 300 metres, and many people believed it impossible.\n",
      "tall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT:  it opened in 1894 and is 158.1 metres (518 ft) tall.\n",
      "levels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT:  the tower has three levels for visitors, with restaurants on the first and second levels.\n",
      "bye\n",
      "BOT: Goodbye and take care!\n"
     ]
    }
   ],
   "source": [
    "continue_chat = True\n",
    "print(\"BOT: Hello! You can ask me anything about the Eiffel Tower.\")\n",
    "while(continue_chat == True):\n",
    "    user_text = input()\n",
    "    user_text = user_text.lower()\n",
    "    if user_text != 'bye':\n",
    "        if user_text == 'thanks' or user_text == 'thank you':\n",
    "            print('BOT: Most welcome!')\n",
    "            continue_chat = False\n",
    "        else:\n",
    "            if greeting_response(user_text) != None:\n",
    "                print('BOT: ', greeting_response(user_text))\n",
    "            else:\n",
    "                print('BOT: ', query_response(user_text))\n",
    "                sentences.remove(user_text) # remove from corpus once responded\n",
    "    else:\n",
    "        print('BOT: Goodbye and take care!')\n",
    "        continue_chat = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
